{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model for Binding Affinity using BindingDB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install dependencies and import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch fair-esm transformers aiondata pymilvus &> /dev/null\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "# Third-party imports for numerical operations and machine learning\n",
    "import joblib\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType\n",
    "\n",
    "# Third-party imports for deep learning and specific models\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Imports from specific libraries used in cheminformatics and bioinformatics\n",
    "from rdkit import RDLogger, Chem\n",
    "import esm\n",
    "from aiondata import BindingAffinity\n",
    "\n",
    "aiondata_path = Path(os.environ.get(\"AIONDATA_CACHE\", \"~/.aiondata\")).expanduser()\n",
    "ligand_dim = 768\n",
    "protein_dim = 480\n",
    "\n",
    "# Only test on n examples, set to None to test on all examples\n",
    "test_only_n_examples = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load previously saved model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "try:\n",
    "    model = joblib.load(aiondata_path / \"models\" / \"binding_affinity.joblib\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Model not found. Please train the model first.\")\n",
    "\n",
    "# Load embeddings local file from PyMilvus\n",
    "client = MilvusClient(str(aiondata_path / \"embeddings.db\"))\n",
    "\n",
    "ligand_schema = CollectionSchema(fields=[\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True),\n",
    "    FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=ligand_dim),\n",
    "    FieldSchema(name=\"inchikey\", dtype=DataType.VARCHAR, max_length=27),\n",
    "])\n",
    "protein_schema = CollectionSchema(fields=[\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True),\n",
    "    FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=protein_dim),\n",
    "    FieldSchema(name=\"sequence\", dtype=DataType.VARCHAR, max_length=1028),\n",
    "])\n",
    "\n",
    "if not client.has_collection(\"ligands\"):\n",
    "    client.create_collection(\"ligands\", dimension=ligand_dim, schema=ligand_schema)\n",
    "if not client.has_collection(\"proteins\"):\n",
    "    client.create_collection(\"proteins\", dimension=protein_dim, schema=protein_schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load BindingAffinity and prepare for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load BindingDB into a Polars DataFrame\n",
    "df = BindingAffinity().to_df()\n",
    "\n",
    "# Filter out rows with missing SMILES, Sequence, or Binds values\n",
    "df = df.drop_nulls(subset=[\"SMILES\", \"Sequence\", \"Binds\"])\n",
    "\n",
    "# For test purposes only use a subset of the data\n",
    "if test_only_n_examples:\n",
    "    df = df.sample(n=test_only_n_examples, shuffle=True, seed=18)\n",
    "\n",
    "# Get the SMILES, Sequence, and Binds columns\n",
    "ligands = df[\"SMILES\"]\n",
    "target_sequence = df[\"Sequence\"]\n",
    "affinity = df[\"Binds\"]\n",
    "\n",
    "# Suppress RDKit warnings and errors\n",
    "RDLogger.DisableLog(\"rdApp.*\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Protein Embeddings using ESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9804ea1b3150445392c11600f5be871f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating protein embeddings:   0%|          | 0/591469 [00:00<?, ? proteins/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load ESM-35m model\n",
    "esm_model, esm_alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "\n",
    "# Prepare model and move it to evaluation mode\n",
    "esm_model = esm_model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    esm_model = esm_model.cuda()\n",
    "\n",
    "id_value = 0\n",
    "\n",
    "def create_protein_embedding(sequence: str):\n",
    "    \"\"\"Generate embeddings for a single protein sequence using a cache to store previous computations.\"\"\"\n",
    "    global id_value\n",
    "    \n",
    "    # Check if the embedding is in the database\n",
    "    query = client.query(collection_name=\"proteins\", filter=f\"sequence == '{sequence}'\")\n",
    "    if query and query[0][\"sequence\"] == sequence:\n",
    "        return np.array(query[0][\"vector\"])\n",
    "\n",
    "    # Convert sequence to tokens\n",
    "    tokens = torch.tensor([esm_alphabet.encode(sequence)])\n",
    "    if torch.cuda.is_available():\n",
    "        tokens = tokens.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = esm_model(tokens, repr_layers=[12])  # Extract embeddings from the last layer\n",
    "\n",
    "    # Extract embeddings and move to cpu\n",
    "    embeddings_full = results[\"representations\"][12].squeeze(0).cpu()\n",
    "\n",
    "    # Reduce the embeddings to 1D by averaging across the sequence length\n",
    "    embedding = embeddings_full.mean(dim=0).numpy()\n",
    "\n",
    "    # Store in database\n",
    "    client.insert(collection_name=\"proteins\", data=[{\"id\": id_value, \"sequence\": sequence, \"vector\": embedding}])\n",
    "    id_value += 1\n",
    "\n",
    "    return embedding\n",
    "\n",
    "def create_embedding_generator(sequences: list[str]):\n",
    "    \"\"\"Generate embeddings for a list of protein sequences.\"\"\"\n",
    "    for sequence in tqdm(sequences, desc=\"Generating protein embeddings\", unit=\" proteins\"):\n",
    "        yield create_protein_embedding(sequence)\n",
    "\n",
    "# Generate embeddings for all protein sequences\n",
    "X_proteins = list(create_embedding_generator(target_sequence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Ligand Embeddings using ChemBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821fc6404aa841499edbdb9cef102f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating ligand embeddings:   0%|          | 0/591469 [00:00<?, ? ligand/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load ChemBERTa model and tokenizer\n",
    "chemberta_tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "chemberta_model = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "chemberta_model.eval()\n",
    "\n",
    "id_value = 0\n",
    "\n",
    "def create_ligand_embedding(smiles: str):\n",
    "    \"\"\"Generate embeddings for a single SMILES string using a cache to store previous computations.\"\"\"\n",
    "    \n",
    "    global id_value\n",
    "\n",
    "    # Check if the embedding is in the database\n",
    "    inchi_key = Chem.MolToInchiKey(Chem.MolFromSmiles(smiles))\n",
    "    query = client.query(collection_name=\"ligands\", filter=f\"inchikey == '{inchi_key}'\")\n",
    "    if query and query[0][\"inchikey\"] == inchi_key:\n",
    "        return np.array(query[0][\"vector\"])\n",
    "    \n",
    "    # Truncate SMILES string to 512 characters, required by ChemBERTa\n",
    "    if len(smiles) > 512:\n",
    "        smiles = smiles[:512]\n",
    "    \n",
    "    inputs = chemberta_tokenizer(smiles, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = chemberta_model(**inputs)\n",
    "    \n",
    "    # Take the mean of the last hidden state to get a single vector representation\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze(0).numpy()\n",
    "    \n",
    "    # Store in database\n",
    "    client.insert(collection_name=\"ligands\", data=[{\"id\": id_value, \"inchikey\": inchi_key, \"vector\": embedding}])\n",
    "    id_value += 1\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def create_embedding_generator(smiles: list[str]):\n",
    "    \"\"\"Generate embeddings for a list of SMILES strings.\"\"\"\n",
    "    for smile in tqdm(smiles, desc=\"Generating ligand embeddings\", unit=\" ligand\"):\n",
    "        yield create_ligand_embedding(smile)\n",
    "\n",
    "# Generate embeddings for all ligands\n",
    "X_ligands = list(create_embedding_generator(ligands))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model and predict binding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score: 0.9863\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate([X_ligands, X_proteins], axis=1)\n",
    "\n",
    "# Make affinity into a numpy y array\n",
    "y = affinity.to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=18)\n",
    "\n",
    "def train_new_model():\n",
    "    # Train a new Random Forest model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=18)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "if not model:\n",
    "    print(\"Model not found. Training a new model.\")\n",
    "    train_new_model()\n",
    "\n",
    "# Evaluate the model ROC-AUC score\n",
    "y_pred = model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f\"ROC-AUC score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank a list of ligands based on probable binding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21c7e4b15b444e8b4ad25e63c176df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating ligand embeddings:   0%|          | 0/26 [00:00<?, ? ligand/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d81a7037e44b3184902706ba08a607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating ligand embeddings:   0%|          | 0/87 [00:00<?, ? ligand/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01d90ac44994ecb92c1592db3531cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating ligand embeddings:   0%|          | 0/41 [00:00<?, ? ligand/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca20906fca25450e8c144a38cc79ff90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating ligand embeddings:   0%|          | 0/51 [00:00<?, ? ligand/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433e3baf84d24fc78ad0fb3eb7acd6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating ligand embeddings:   0%|          | 0/33 [00:00<?, ? ligand/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ec3a8b64064dc9be57bcaf5801ee7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating ligand embeddings:   0%|          | 0/34 [00:00<?, ? ligand/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cc424a28fa4e1a91c99bac3807938e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating ligand embeddings:   0%|          | 0/24 [00:00<?, ? ligand/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5183eba2236e4c8cb7b75f77b92e4241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating ligand embeddings:   0%|          | 0/53 [00:00<?, ? ligand/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (8, 5)\n",
      "┌──────────────────────────┬────────────────────────┬─────────────┬────────────────┬───────────────┐\n",
      "│ Target                   ┆ Predicted              ┆ MSE         ┆ Spearman's Rho ┆ Kendall's Tau │\n",
      "│ ---                      ┆ ---                    ┆ ---         ┆ ---            ┆ ---           │\n",
      "│ str                      ┆ str                    ┆ f64         ┆ f64            ┆ f64           │\n",
      "╞══════════════════════════╪════════════════════════╪═════════════╪════════════════╪═══════════════╡\n",
      "│ shp2                     ┆ 10, 17, 1, 7, 16, 12   ┆ 88.461538   ┆ 0.213675       ┆ 0.144615      │\n",
      "│ pfkfb3_automap           ┆ 20, 68, 67, 76, 75, 25 ┆ 1655.563218 ┆ -0.31255       ┆ -0.232291     │\n",
      "│ cdk8_5cei_new_helix_loop ┆ 1, 2, 7, 8, 9, 10      ┆ 119.219512  ┆ 0.574216       ┆ 0.429268      │\n",
      "│ _extra                   ┆                        ┆             ┆                ┆               │\n",
      "│ hif2a_automap            ┆ 25, 24, 10, 5, 40, 39  ┆ 533.490196  ┆ -0.231131      ┆ -0.162353     │\n",
      "│ tnks2_fullmap            ┆ 8, 1, 15, 6, 24, 4     ┆ 135.333333  ┆ 0.253676       ┆ 0.147727      │\n",
      "│ eg5_extraprotomers       ┆ 33, 23, 22, 20, 30, 31 ┆ 306.470588  ┆ -0.592055      ┆ -0.411765     │\n",
      "│ cmet                     ┆ 7, 15, 2, 1, 20, 18    ┆ 95.583333   ┆ 0.002609       ┆ -0.014493     │\n",
      "│ syk_4puz_fullmap         ┆ 51, 43, 11, 40, 27, 28 ┆ 683.396226  ┆ -0.460248      ┆ -0.300435     │\n",
      "└──────────────────────────┴────────────────────────┴─────────────┴────────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "def rank_binding_affinity(list_of_ligands: Iterable[str], target_sequence: str):\n",
    "    \"\"\"Rank the binding affinity of a list of ligands to a target protein sequence.\"\"\"\n",
    "    X_ligands = np.array(list(create_embedding_generator(list_of_ligands)))\n",
    "    X_protein = np.array([create_protein_embedding(target_sequence)])\n",
    "    # Duplicate the protein embedding for each ligand\n",
    "    X_protein = np.tile(X_protein, (X_ligands.shape[0], 1))\n",
    "    X = np.concatenate([X_ligands, X_protein], axis=1)\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # sort by binding confidence\n",
    "    ranked_indices = np.argsort(y_pred)[::-1]\n",
    "    ranked_ligands = np.array(list_of_ligands)[ranked_indices]\n",
    "    ranked_scores = y_pred[ranked_indices]\n",
    "\n",
    "    return ranked_indices, ranked_ligands, ranked_scores\n",
    "\n",
    "def calculate_metrics(true_ranks, predicted_ranks):\n",
    "    mse = mean_squared_error(true_ranks, predicted_ranks)\n",
    "    spearman_corr, _ = spearmanr(true_ranks, predicted_ranks)\n",
    "    kendall_tau, _ = kendalltau(true_ranks, predicted_ranks)\n",
    "    \n",
    "    return {\"MSE\": mse, \"Spearman's Rho\": spearman_corr, \"Kendall's Tau\": kendall_tau}\n",
    "\n",
    "def create_ranked_ligand_df(smiles_csv, rank_csv, fasta_file):\n",
    "    \"\"\"Create a DataFrame with ranked ligands and their scores.\"\"\"\n",
    "    smiles_df = pl.read_csv(smiles_csv, separator=\"\\t\") \n",
    "    rank_df = pl.read_csv(rank_csv, has_header=False, new_columns=[\"ligand\", \"rank\"])\n",
    "\n",
    "    rank_df = rank_df.with_columns([\n",
    "        pl.col(\"rank\").str.strip_chars().cast(pl.Float64)\n",
    "    ])\n",
    "\n",
    "    # Combine the two DataFrames on ligand\n",
    "    merged_df = smiles_df.join(rank_df, on=\"ligand\")\n",
    "\n",
    "    # Load the target sequence from a FASTA file\n",
    "    with open(fasta_file) as f:\n",
    "        target_sequence = \"\".join(line.strip() for line in f.readlines()[1:])\n",
    "\n",
    "    return merged_df, target_sequence\n",
    "\n",
    "\n",
    "def walk_every_target_subdir_and_do_ranking(root_dir):\n",
    "    \"\"\"Walk through every subdirectory in the root directory and perform ranking.\"\"\"\n",
    "    for subdir in os.listdir(root_dir):\n",
    "        subdir_path = root_dir / subdir\n",
    "        if not subdir_path.is_dir():\n",
    "            continue\n",
    "\n",
    "        smiles_csv = subdir_path / f\"{subdir}_smiles.csv\"\n",
    "        rank_csv = subdir_path / f\"{subdir}_rank.csv\"\n",
    "        fasta_file = subdir_path / f\"{subdir}_fasta\"\n",
    "\n",
    "        test_df, target_sequence = create_ranked_ligand_df(smiles_csv, rank_csv, fasta_file)\n",
    "        ligands = test_df[\" Canonical SMILES \"]\n",
    "\n",
    "        y_pred, ranked_ligands, ranked_scores = rank_binding_affinity(ligands, target_sequence)\n",
    "\n",
    "        # y_true is merely the indices of the df\n",
    "        y_true = np.arange(len(ligands))\n",
    "\n",
    "        result = {\"Target\": subdir, \"Predicted\": \", \".join([str(p) for p in y_pred[:6]])}\n",
    "        result.update(calculate_metrics(y_true, y_pred))\n",
    "\n",
    "        yield result\n",
    "\n",
    "\n",
    "print(pl.DataFrame(walk_every_target_subdir_and_do_ranking(aiondata_path / \"schrodinger-fepp\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model and embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_save_path = aiondata_path / \"models\"\n",
    "model_save_path.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(model, model_save_path / \"binding_affinity.joblib\")\n",
    "\n",
    "# Close the embeddings\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiondata-mTfynZ7s-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
